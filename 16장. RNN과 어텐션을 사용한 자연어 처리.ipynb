{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"16. RNN과 어텐션을 사용한 자연어 처리.ipynb","provenance":[],"collapsed_sections":["9CZA-H0780B5","0nso2j2B80B6","8Jl2iNkV80B7","_H0bYOcl80B8","GM_wqrXgTCob"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9XcJ0Rn480Bu"},"source":["### 1. Char-RNN을 사용해 셰익스피어 문체 생성하기"]},{"cell_type":"markdown","metadata":{"id":"LIZzYoOp80Bz"},"source":["#### 1) 훈련 데이터셋 만들기"]},{"cell_type":"code","metadata":{"id":"Lqd-3u6980B0","executionInfo":{"status":"ok","timestamp":1612624423009,"user_tz":-540,"elapsed":741,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["from tensorflow import keras\n","import tensorflow as tf\n","import numpy as np"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLf3ePkX80B1","executionInfo":{"status":"ok","timestamp":1612334574458,"user_tz":-540,"elapsed":1901,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"6a26421e-1fcf-428f-9ff6-68b3dcff9e17"},"source":["# 데이터 불러오기\n","\n","shakespeare_url = 'https://homl.info/shakespeare'\n","filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n","\n","with open(filepath) as f:\n","    shakespeare_text = f.read()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://homl.info/shakespeare\n","1122304/1115394 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"jlMDW8uQ80B2","executionInfo":{"status":"ok","timestamp":1612334574462,"user_tz":-540,"elapsed":1115,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"44fcd91f-569f-4dcb-d39e-fe620392b3fc"},"source":["shakespeare_text[:50]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'First Citizen:\\nBefore we proceed any further, hear'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ypTc2Sh80B3","executionInfo":{"status":"ok","timestamp":1612334574464,"user_tz":-540,"elapsed":801,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"86f4a5e3-aec3-4054-eaf8-752ae661a458"},"source":["len(shakespeare_text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1115394"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"tUCjx0y980B3"},"source":["# 글자를 정수(ID)로 인코딩하기 (ID는 1부터 시작)\n","\n","tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # 글자수준으로\n","tokenizer.fit_on_texts(shakespeare_text)  # 이 텍스트에 훈련하기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmyikXqT80B3","executionInfo":{"status":"ok","timestamp":1612334575920,"user_tz":-540,"elapsed":1709,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"f10bf936-95cc-4350-f366-15b5c9a693f1"},"source":["# tokenizer 작동 확인\n","\n","tokenizer.texts_to_sequences(['First'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[20, 6, 9, 8, 3]]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"voBgApED80B3","executionInfo":{"status":"ok","timestamp":1612334575922,"user_tz":-540,"elapsed":1379,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"35d51f07-f22a-4680-9601-6ed8be5dba46"},"source":["tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['f i r s t']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GExg2Sj880B4","executionInfo":{"status":"ok","timestamp":1612334575923,"user_tz":-540,"elapsed":1038,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"bf69c759-95c5-494c-80bb-2618f04cbb1c"},"source":["max_id = len(tokenizer.word_index) # 고유 글자 개수\n","dataset_size = tokenizer.document_count # 전체 글자 개수\n","\n","max_id, dataset_size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(39, 1115394)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"wzAAqYSk80B4"},"source":["# 전체 텍스트를 인코딩하여 글자를 ID로 나타내기\n","# ID를 0부터 시작하기 위해 1을 빼줌\n","\n","[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n","\n","# [encoded]를 해주는 이유 : 그냥 encoded하면 [[19, ...]]이렇게 되는데,\n","# 리스트 하나를 빼주려고"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lXsqBFiG80B4","executionInfo":{"status":"ok","timestamp":1612334577324,"user_tz":-540,"elapsed":451,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"a4a81873-db1c-4def-f897-6c6e55b616c2"},"source":["encoded"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([19,  5,  8, ..., 20, 26, 10])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"563YWHxm80B5"},"source":["#### 2) 순차 데이터셋 나누기"]},{"cell_type":"code","metadata":{"id":"_9OCnrSV80B5"},"source":["# 90%를 훈련 데이터셋으로 사용하기\n","\n","train_size = dataset_size * 90 // 100\n","dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","  # 한 번에 한 글자씩 반환하는 tf.data.Dataset 객체 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0LaRetJ80B5"},"source":["# tf.data.Dataset.from_tensor_slices 작동 방식\n","\n","# t = tf.constant([[1, 2], [3, 4]])\n","# ds = tf.data.Dataset.from_tensors(t)   # [[1, 2], [3, 4]]\n","\n","# t = tf.constant([[1, 2], [3, 4]])\n","# ds = tf.data.Dataset.from_tensor_slices(t)   # [1, 2], [3, 4]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9CZA-H0780B5"},"source":["#### 3) 순차 데이터를 윈도 여러개로 자르기\n","<img src='img/16_1.png' width='400'>"]},{"cell_type":"code","metadata":{"id":"N3NLWz0A80B5"},"source":["### 윈도로 끊어주기\n","\n","n_steps = 100\n","window_length = n_steps + 1\n","dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n","\n","## drop_remainder=False 이면\n","# ds = tf.data.Dataset.range(6) \n","# ds = ds.window(5, shift=1, drop_remainder=False)\n","# for d in ds:\n","#     print(list(d.as_numpy_iterator()))\n","\n","# [[0, 1, 2, 3, 4],\n","# [1, 2, 3, 4, 5],\n","# [2, 3, 4, 5],\n","# [3, 4, 5],\n","# [4, 5],\n","# [5]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swcdiS7n80B6"},"source":["### 중첩 데이터(nested data)를 flat 데이터로 만들어주기\n","\n","# 이때, batch() 함수를 사용해 윈도 길이로 끊어주기\n","# {{1, 2}, {3, 4, 5, 6}} (중첩 data) -> {[1, 2], [3, 4], [5, 6]} (flat data)\n","\n","dataset = dataset.flat_map(lambda window: window.batch(window_length))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nb4C70c380B6"},"source":["### 훈련 data 섞고, 배치로 만든 후, X와 y 분리하기\n","# 섞는 이유 : data가 iid일 때, 경사하강법이 가장 잘 작동하기 때문\n","\n","batch_size = 32\n","dataset = dataset.shuffle(10000).batch(batch_size)\n","    # shuffle의 파라미터는 buffer_size 적당히 큰 수를 해줘야 랜덤하게 잘 뽑음\n","dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ADzt1Bsn80B6"},"source":["### 원핫 인코딩\n","\n","dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","dataset = dataset.prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nso2j2B80B6"},"source":["#### 4) 모델만들고 훈련하기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTFiJ3ya80B7","executionInfo":{"elapsed":15794,"status":"ok","timestamp":1612239365438,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"},"user_tz":-540},"outputId":"6bdaf8b0-15fe-425c-ba2d-fd113ba373c5"},"source":["model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                    dropout=0.2, recurrent_dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                    dropout=0.2, recurrent_dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n","])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xC2OmkmM80B7"},"source":["model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7k4PaGRI80B7","scrolled":true,"outputId":"5c3913c5-6d84-4e31-a37f-29b3e7792940"},"source":["# 오래 걸림\n","\n","history = model.fit(dataset, epochs=20) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","  21905/Unknown - 12710s 580ms/step - loss: 1.6434"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8Jl2iNkV80B7"},"source":["#### 5) 모델 사용하기"]},{"cell_type":"code","metadata":{"id":"X6zleKwA80B7"},"source":["def preprocess(text):\n","    X = np.array(tokenizer.text_to_sequences(text)) - 1\n","    return tf.one_hot(X, max_id)\n","\n","# 토큰화 해주고, 원핫인코딩 해주기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGUusUvE80B8"},"source":["# 에측하기\n","\n","X_new = preprocess(['How are yo'])\n","Y_pred = model.predict_classes(X_new)\n","tokenizer.sequences_to_text(Y_pred + 1)[0][-1]  # 첫번째 문장, 마지막 글자"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_H0bYOcl80B8"},"source":["#### 6) 가짜 셰익스피어 텍스트 생성하기"]},{"cell_type":"code","metadata":{"id":"75G7p3Dp80B8"},"source":["# 다음 글자 예측하는 함수\n","\n","def next_chat(text, temperature=1):\n","        # temperature이 0에 가까울수록 높은 확률을 가진 글자를 선택 \n","        # (확률분포를 더 두드러지게, 1에 가까우면 그냥 원래의 확률분포로)\n","        # (원래 predict는 0~1의 출력. 여기에 로그를 취하고 0에 가까운 temperature로 \n","        # 나눈 후 다시 지수 함수로 복원하면 작았던 확률이 더 크게 작아짐)\n","\n","    X_new = preprocess([text])\n","    y_proba = model.predict(X_new)[0, -1:, :]\n","    rescale_logits = tf.math.log(y_proba) / temperature\n","    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n","        # 추정한 확률(로짓)을 기반으로 랜덤하게 클래스를 샘플링\n","        # 이렇게 안하면 같은 단어가 계속 반복되는 경우가 많음\n","    return tokenizer.sequences._to_text(char_id.numpy())[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRdw7rAL9D_j"},"source":["# next_char을 반복 호출하여 텍스트에 추가해나가는 함수\r\n","\r\n","def complete_text(text, n_chars=50, temperature=1):\r\n","    for _ in range(n_chars):\r\n","        text += next_chart(text, temperatrue)\r\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9KXgqOFHq8z"},"source":["print(complete_text('t', temperature=0.2), '\\n')\r\n","print(complete_text('w', temperature=1), '\\n')\r\n","print(complete_text('w', temperature=2), '\\n')\r\n","\r\n","# 여기서는 1에 가까운 온동에서 잘 작동함\r\n","# 더 좋은 텍스트를 생성하려면 GRU층과 층의 뉴런 수를 늘리고 더 오래 훈련하거나 규제를 추가"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GM_wqrXgTCob"},"source":["#### 7) 상태가 있는 RNN (stateful RNN)\r\n","- 입력 데이터가 순차적이고 겹치기 않아야!  \r\n","  ex) 첫 번째 배치는 윈도 1 ~ 32, 두 번째 배치는 윈도 33 ~ 64  \r\n","  <img src='img/16_2.png' width='400'>"]},{"cell_type":"code","metadata":{"id":"CPQlAEzLItYj"},"source":["dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\r\n","dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\r\n","dataset = dataset.flat_map(lambda window: window.batch(window_length))\r\n","dataset = dataset.batch(1)\r\n","dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\r\n","dataset = dataset.map(\r\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\r\n","dataset = dataset.prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiSxlOIFFBan"},"source":["# 배치만드려면 아래와 같은 코드 (데이터를 32개로 나누기)\r\n","\r\n","# batch_size = 32\r\n","# encoded_parts = np.array_split(encoded[:train_size], batch_size)\r\n","\r\n","# datasets = []\r\n","# for encoded_part in encoded_parts:\r\n","#     dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\r\n","#     dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\r\n","#     dataset = dataset.flat_map(lambda window: window.batch(window_length))\r\n","#     datasets.append(dataset)\r\n","# dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\r\n","# dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\r\n","# dataset = dataset.map(\r\n","#     lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\r\n","# dataset = dataset.prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UZnH6OOWASE"},"source":["# 모델 구성 및 학습\r\n","\r\n","model = keras.models.Sequential([\r\n","    keras.layers.GRU(128, return_sequences=True, stateful=True,\r\n","                     dropout=0.2, recurrent_dropout=0.2,\r\n","                     batch_input_shape=[1, None, max_id]),\r\n","    keras.layers.GRU(128, return_sequences=True, stateful=True,\r\n","                     dropout=0.2, recurrent_dropout=0.2),\r\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\r\n","                                                    activation='softmax'))\r\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6btYM45cWgOp"},"source":["# 모델 재설정해주는 콜백함수\r\n","\r\n","class ResetStatesCallback(keras.callbacks.Callback):\r\n","    def on_epoch_begin(self, epoch, logs):\r\n","        self.model.reset_states()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDTAyEEfNp6N"},"source":["model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CorY4nGOF7T"},"source":["steps_per_epoch = train_size // batch_size // n_steps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"sqc7hvuNNySu","outputId":"0949232c-2ed1-4b85-b219-8f72af236993"},"source":["history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\r\n","                    callbacks=[ResetStatesCallback()])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","313/313 [==============================] - 75s 229ms/step - loss: 3.0333\n","Epoch 2/50\n","313/313 [==============================] - 71s 226ms/step - loss: 2.4841\n","Epoch 3/50\n","313/313 [==============================] - 71s 228ms/step - loss: 2.2787\n","Epoch 4/50\n","313/313 [==============================] - 74s 236ms/step - loss: 2.2314\n","Epoch 5/50\n","313/313 [==============================] - 70s 223ms/step - loss: 2.1705\n","Epoch 6/50\n","313/313 [==============================] - 69s 219ms/step - loss: 2.1746\n","Epoch 7/50\n","313/313 [==============================] - 70s 222ms/step - loss: 2.0817\n","Epoch 8/50\n","313/313 [==============================] - 69s 222ms/step - loss: 2.0439\n","Epoch 9/50\n","313/313 [==============================] - 70s 224ms/step - loss: 2.0255\n","Epoch 10/50\n","313/313 [==============================] - 70s 222ms/step - loss: 1.8959\n","Epoch 11/50\n","313/313 [==============================] - 69s 221ms/step - loss: 2.0144\n","Epoch 12/50\n","313/313 [==============================] - 69s 219ms/step - loss: 1.9935\n","Epoch 13/50\n","313/313 [==============================] - 69s 221ms/step - loss: 1.9628\n","Epoch 14/50\n","313/313 [==============================] - 69s 222ms/step - loss: 1.9117\n","Epoch 15/50\n","313/313 [==============================] - 71s 226ms/step - loss: 1.9826\n","Epoch 16/50\n","313/313 [==============================] - 73s 232ms/step - loss: 1.9593\n","Epoch 17/50\n","313/313 [==============================] - 71s 227ms/step - loss: 1.9308\n","Epoch 18/50\n","313/313 [==============================] - 70s 224ms/step - loss: 1.8579\n","Epoch 19/50\n","313/313 [==============================] - 70s 225ms/step - loss: 1.8837\n","Epoch 20/50\n","313/313 [==============================] - 72s 229ms/step - loss: 1.9260\n","Epoch 21/50\n","313/313 [==============================] - 74s 236ms/step - loss: 1.8879\n","Epoch 22/50\n","313/313 [==============================] - 71s 227ms/step - loss: 1.7817\n","Epoch 23/50\n","313/313 [==============================] - 71s 226ms/step - loss: 1.8011\n","Epoch 24/50\n","313/313 [==============================] - 74s 235ms/step - loss: 1.9389\n","Epoch 25/50\n","313/313 [==============================] - 71s 225ms/step - loss: 1.8860\n","Epoch 26/50\n","313/313 [==============================] - 70s 225ms/step - loss: 1.9050\n","Epoch 27/50\n","313/313 [==============================] - 71s 226ms/step - loss: 1.8130\n","Epoch 28/50\n","313/313 [==============================] - 72s 228ms/step - loss: 1.8865\n","Epoch 29/50\n","313/313 [==============================] - 74s 236ms/step - loss: 1.8062\n","Epoch 30/50\n","313/313 [==============================] - 72s 232ms/step - loss: 1.7948\n","Epoch 31/50\n","313/313 [==============================] - 69s 222ms/step - loss: 1.7503\n","Epoch 32/50\n","313/313 [==============================] - 69s 221ms/step - loss: 1.8923\n","Epoch 33/50\n"," 22/313 [=>............................] - ETA: 1:04 - loss: 1.7862WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15650 batches). You may need to use the repeat() function when building your dataset.\n","313/313 [==============================] - 5s 15ms/step - loss: 1.8319\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JzF8WlV0QKBP"},"source":["### 감성 분석\r\n","- IMDb 리뷰 데이터 (영화 리뷰 50,000개 & 이진 분류 - 0(부정)/1(긍정))"]},{"cell_type":"code","metadata":{"id":"tp-csI-qQfhk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612416898918,"user_tz":-540,"elapsed":6179,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"ea24b23d-592e-4898-9d93-25426565f885"},"source":["# 데이터 적재\r\n","\r\n","(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\r\n","X_train[0][:10]  # 전처리 되서 넘파이 정수 배열로 표현됨\r\n","                 # 구두점을 모두 제거하고 단어는 소문자로 변환한 다음, 공백으로 나누어\r\n","                 # 빈도에 따라 인덱스를 붙임 (낮은 정수가 자주 등장하는 단어)\r\n","                 # 단, 0:패딩토큰, 1:SOS토큰, 2:알수없는 단어"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"xdtXS9LeQmwH","colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"status":"ok","timestamp":1612416898919,"user_tz":-540,"elapsed":3269,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"7e833e4b-0abe-4e76-b1b8-8d3ec19d04fa"},"source":["# 리뷰 내용을 보기위해 디코딩해보기\r\n","\r\n","word_index = keras.datasets.imdb.get_word_index()\r\n","id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\r\n","\r\n","# word_index에는 단어들로만 이루어져 있는데(1:'the'), 실제 인코딩은 1:<sos>임\r\n","# 디코딩해주기 위해서 0, 1, 2에 해당하는 토큰들도 넣어주기\r\n","for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\r\n","    id_to_word[id_] = token\r\n","\r\n","' '.join([id_to_word[id_] for id_ in X_train[0][:10]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<sos> this film was just brilliant casting location scenery story'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"NWgEo4gLpTrT","executionInfo":{"status":"ok","timestamp":1612624428325,"user_tz":-540,"elapsed":708,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["# 전처리를 모델 자체에 포함시키기\r\n","\r\n","import tensorflow_datasets as tfds"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"PudKSyq2qaDu","executionInfo":{"status":"ok","timestamp":1612624416873,"user_tz":-540,"elapsed":2773,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\r\n","    # as_supervised=True하면 데이터가 (input, label)형태로 나옴\r\n","train_size = info.splits['train'].num_examples"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ASc-yOGiE7X","executionInfo":{"status":"ok","timestamp":1612624416874,"user_tz":-540,"elapsed":2321,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["### 전처리 함수\r\n","\r\n","def preprocess(X_batch, y_batch):\r\n","    X_batch = tf.strings.substr(X_batch, 0, 300)  # 리뷰에서 처음 300글자만 남김 (속도 up)\r\n","    X_batch = tf.strings.regex_replace(X_batch, b'<br\\\\s*/?>', b' ') # <br /> 공백으로\r\n","    X_batch = tf.strings.regex_replace(X_batch, b'[^a-zA-Z]', b' ')  # 문자, 작은따옴표아니면 공백으로\r\n","    X_batch = tf.strings.split(X_batch)  # 띄어쓰기 기준으로 split (래그드 텐서가 반환됨)\r\n","    return X_batch.to_tensor(default_value=b'<pad'), y_batch  # 밀집 텐서(일반 텐서)로 바꾸고 \r\n","                                # 동일한 길이의 텐서가 되도록 <pad> 토큰으로 패딩하기\r\n","                                # default_value를 지정하지 않으면 빈 바이트 문자열로 패딩"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7kZCZEWh_Hx","executionInfo":{"status":"ok","timestamp":1612624416875,"user_tz":-540,"elapsed":1820,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["### 어휘사전 구축하기\r\n","\r\n","from collections import Counter\r\n","vocabulary = Counter()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPXcAfokur2p","executionInfo":{"status":"ok","timestamp":1612624450811,"user_tz":-540,"elapsed":5674,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\r\n","                                        # batch 사이즈가 32\r\n","    for review in X_batch:\r\n","        vocabulary.update(list(review.numpy())) # 배치마다 리스트로 업데이트"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8ll2EdHiPc-","executionInfo":{"status":"ok","timestamp":1612624457304,"user_tz":-540,"elapsed":435,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"6246b197-fd17-463b-dced-912f1aea3987"},"source":["vocabulary.most_common()[:3]  # 가장 많이 등장하는 단어 한 번 확인해보기"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(b'<pad', 224494), (b'the', 61156), (b'a', 38569)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"oJ4Bq84PihQW","executionInfo":{"status":"ok","timestamp":1612624457829,"user_tz":-540,"elapsed":646,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["### 어휘 사전에 가장 많이 등장하는 단어 10,000개만 남기고 삭제하기\r\n","\r\n","vocab_size = 10000\r\n","truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"NU6JT636jA3X","executionInfo":{"status":"ok","timestamp":1612624458449,"user_tz":-540,"elapsed":1011,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["### oov 버킷을 사용하는 룩업 테이블 만들기\r\n","\r\n","words = tf.constant(truncated_vocabulary)\r\n","word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\r\n","vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\r\n","        # 범주 리스트와 해당 인덱스를 전달하여 룩업 테이블을 위한 초기화 객체 만들기\r\n","num_oov_buckets = 1000\r\n","table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\r\n","        # 초기화 객체와 버킷을 지정하여 룩업 테이블 만들기\r\n","        # 어휘 사전에 없는 범주를 찾으면 룩업 테이블에 추가해주기\r\n","        # 최대 추가 개수는 1000개"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flp-CQz-mXVW","executionInfo":{"status":"ok","timestamp":1612624459246,"user_tz":-540,"elapsed":486,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"46fa86c3-c87d-4493-a31c-aefc9d09fd70"},"source":["# 단어 몇 개에 대한 ID 확인해보기\r\n","\r\n","table.lookup(tf.constant([b'This movie was faaaaaantastic'.split()]))\r\n","\r\n","# TF 변환에서 이런 어휘 사전 편리하게 다룰 수 있는 함수 제공\r\n","# tft.compute_and_apply_vocabulary로 고유한 모든 단어 찾아 어휘 사전 구축"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   24,    12,    13, 10053]])>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"cPOieuAJm9bf","executionInfo":{"status":"ok","timestamp":1612624459575,"user_tz":-540,"elapsed":643,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["### 최종 훈련 세트 만들기\r\n","\r\n","def encode_words(X_batch, y_batch):\r\n","    return table.lookup(X_batch), y_batch\r\n","\r\n","train_set = datasets['train'].batch(32).map(preprocess)\r\n","train_set = train_set.map(encode_words).prefetch(1)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZZl_fzdnUsl","executionInfo":{"status":"ok","timestamp":1612624460197,"user_tz":-540,"elapsed":1083,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["### 모델 만들기\r\n","\r\n","embed_size = 128  # 임베딩 행렬의 열이 128개\r\n","model = keras.models.Sequential([\r\n","    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\r\n","                           input_shape=[None]),\r\n","        # 입력은 [batch size, timesteps] -> 출력은 [batch size, timesteps, embed_size]\r\n","    keras.layers.GRU(128, return_sequences=True),\r\n","    keras.layers.GRU(128),\r\n","    keras.layers.Dense(1, activation='sigmoid')\r\n","])\r\n","\r\n","# 모델이 패딩 토큰을 무시하도록 학습시키려면 mask_zero=True로 추가해야!\r\n","# Sequential 모델이 아니면 직접 마스킹을 계사해서 다음 층에 전달해야!"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfEU73aAo5Ss","executionInfo":{"status":"ok","timestamp":1612624463176,"user_tz":-540,"elapsed":2050,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["### 함수형 API에서 직접 마스킹 처리하기\r\n","\r\n","K = keras.backend\r\n","inputs = keras.layers.Input(shape=[None])\r\n","mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\r\n","z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\r\n","z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\r\n","z = keras.layers.GRU(128)(z, mask=mask)\r\n","outputs = keras.layers.Dense(1, activation='sigmoid')(z)\r\n","model = keras.Model(inputs=[inputs], outputs=[outputs])"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ThLEU9lFxMgs"},"source":["#### 2) 사전훈련된 임베딩 재사용하기\r\n","- 텐서플로 허브 프로젝트는 사전훈련된 모델 컴포넌트(모듈)를 모델에 추가하기 쉽게 만들어줌"]},{"cell_type":"code","metadata":{"id":"MxUhrwCZrTVc","executionInfo":{"status":"ok","timestamp":1612624417845,"user_tz":-540,"elapsed":809,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["import tensorflow_hub as hub"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPX91ZPqyBYT"},"source":["model = keras.Sequential([\r\n","    hub.KerasLayer('https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1',\r\n","                   dtype=tf.string, input_shape=[], output_shape=[50]), \r\n","                   # 문장 인코더 (문자열을 입력으로 받아 하나의 벡터로 (50차원))\r\n","                   # 모든 단어의 임베딩의 평균을 계산하여 문장 임베딩을 출력\r\n","                   # 이 층은 훈련되지 않음 (옵션 추가해서 할 수는 있음)\r\n","    keras.layers.Dense(128, activation='relu'),\r\n","    keras.layers.Dense(1, activation='sigmoid')\r\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FkLV8zm90EHq"},"source":["# IMDb 데이터를 전처리할 필요가 없고, 바로 모델을 훈련할 수 있음\r\n","\r\n","datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\r\n","train_size = info.splits['train'].num_examples\r\n","batch_size = 32\r\n","train_set = datasets['train'].batch(batch_size).prefetch(1)\r\n","history = model.fit(train_set, epochs=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gpsQWN7t6PID"},"source":["### 3. 신경망 기계 번역을 위한 인코더-디코더 네트워크"]},{"cell_type":"code","metadata":{"id":"pNzI_R945vsF","executionInfo":{"status":"ok","timestamp":1612626580371,"user_tz":-540,"elapsed":693,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["# !pip install tensorflow-addons==0.9.1  # 아래코드는 이 버전으로 해야!"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSBjpTPj6TXi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612624469410,"user_tz":-540,"elapsed":803,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}},"outputId":"572c98e8-86b7-4fcb-c2b5-fa42b9cd0301"},"source":["import tensorflow_addons as tfa\r\n","import numpy as np"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.1.0 and strictly below 2.3.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  UserWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3WpAu5j06XbF","executionInfo":{"status":"ok","timestamp":1612624548233,"user_tz":-540,"elapsed":1002,"user":{"displayName":"희지","photoUrl":"","userId":"04407225407236767854"}}},"source":["encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\r\n","decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\r\n","sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\r\n","\r\n","embeddings = keras.layers.Embedding(vocab_size, embed_size)\r\n","    # input_dim : 단어 사전의 크기, output_dim : 임베딩 벡터의 크기\r\n","    # input_length : 입력 시퀀스의 길이 (다음에 플래튼 레이어가 온다면 반드시 지정해줘야!)\r\n","encoder_embeddings = embeddings(encoder_inputs)\r\n","decoder_embeddings = embeddings(decoder_inputs)\r\n","\r\n","encoder = keras.layers.LSTM(512, return_state=True)  \r\n","    # 최종 은닉 상태를 디코더로 보내기위해 return_state=True\r\n","    # LSTM을 사용하기 때문에 은닉상태 두 개(c_t, h_t)를 반환\r\n","encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\r\n","encoder_state = [state_h, state_c]\r\n","\r\n","sampler = tfa.seq2seq.sampler.TrainingSampler()  # 여러 샘플러 중 하나\r\n","    # 이 샘플러는 각 스텝에서 디코더에게 이전 스텝의 출력이 무엇인지 알려줌\r\n","    # 예측 시에는 실제로 출력되는 토큰의 임베딩\r\n","    # 훈련 시에는 이전 타깃 토큰의 임베딩\r\n","\r\n","    # 실전에서는 ScheduledEmbeddingTrainingSampler와 같이 처음에는 이전 스텝의 \r\n","    # 타깃의 임베딩을 사용해 훈련을 시작해서 점차 출력된 토큰의 임베딩으로 바꾸는게 좋음\r\n","\r\n","decoder_cell = keras.layers.LSTMCell(512)\r\n","output_layer = keras.layers.Dense(vocab_size)\r\n","decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\r\n","                                                 output_layer = output_layer)\r\n","final_outputs, final_state, final_sequence_lengths = decoder(\r\n","    decoder_embeddings, initial_state=encoder_state,\r\n","    sequence_length=sequence_lengths)\r\n","Y_proba = tf.nn.softmax(final_outputs.rnn_output)\r\n","\r\n","model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\r\n","                    outputs=[Y_proba])\r\n","    # decoder_inputs이 들어가는 이유는 훈련시에 이전스텝의 실제값을 넣어주기 때문\r\n","    # 만약, 훈련시에 이전스텝의 출력값을 넣어주면 잘못된 예측이 연쇄적으로 일어나 \r\n","    # 속도가 느려짐 -> 실제값을 넣어줌!"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qLRuaFwY5KJw"},"source":["#### 1) 양방향 RNN\r\n","- 문맥을 이해하여 단어의 의미를 파악하기 위해 반대방향으로 층을 하나 더 만듦"]},{"cell_type":"code","metadata":{"id":"uyU_-jwX5JTo"},"source":["keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WgaKtp_t6Lx7"},"source":["#### 2) 빔 검색\r\n","- k개의 가능성 있는 문장의 리스트와 조건부확률을 유지하고 단계마다 k개의 문장을 만듦"]},{"cell_type":"code","metadata":{"id":"7as4CNrP4d1V"},"source":["beam_width = 10\r\n","decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\r\n","    cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)\r\n","decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(\r\n","    encoder_state, multiplier=beam_width)\r\n","outputs, _, _ = decoder(\r\n","    embedding_decoder, start_tokens=start_tokens, end_token=end_token,\r\n","    initial_state=decoder_initial_state\r\n",")"],"execution_count":null,"outputs":[]}]}