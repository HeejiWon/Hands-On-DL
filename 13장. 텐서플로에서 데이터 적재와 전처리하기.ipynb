{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 API\n",
    "- 일반적으로 디스크에서 데이터를 점진적으로 읽는 데이터셋 사용\n",
    "- 간단히 tf.data.Dataset.from_tensor_slices()를 사용해 메모리에서 데이터셋 생성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.data.Dataset.from_tensor_slices()\n",
    "# 각 원소가 아이템으로 표현되는 tf.data.Dataset 만들기\n",
    "\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X) # tf.data.Dataset.range(10) 동일\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:   # 아이템 순회가능\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 연쇄 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 3번 반복하고 7개씩 묶어서 item으로 만들기\n",
    "\n",
    "dataset = dataset.repeat(3).batch(7)  # drop_reminder=True 하면 길이부족 배치 버림\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 변환\n",
    "\n",
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.unbatch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터 셔플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)  # 3번 반복\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "   # 반복마다 동일한 순서사용 -> reshuffle_each_iteration=False\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 여러 파일에서 한 줄씩 번갈아 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# housing = fetch_california_housing()\n",
    "# X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "#     housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_mean = scaler.mean_\n",
    "# X_std = scaler.scale_\n",
    "\n",
    "# def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "#     housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "#     os.makedirs(housing_dir, exist_ok=True)\n",
    "#     path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "#     filepaths = []\n",
    "#     m = len(data)\n",
    "#     for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "#         part_csv = path_format.format(name_prefix, file_idx)\n",
    "#         filepaths.append(part_csv)\n",
    "#         with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "#             if header is not None:\n",
    "#                 f.write(header)\n",
    "#                 f.write(\"\\n\")\n",
    "#             for row_idx in row_indices:\n",
    "#                 f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "#                 f.write(\"\\n\")\n",
    "#     return filepaths\n",
    "\n",
    "\n",
    "# train_data = np.c_[X_train, y_train]\n",
    "# valid_data = np.c_[X_valid, y_valid]\n",
    "# test_data = np.c_[X_test, y_test]\n",
    "# header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "# header = \",\".join(header_cols)\n",
    "\n",
    "# train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "# valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "# test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\housing\\\\my_train_00.csv',\n",
       " 'datasets\\\\housing\\\\my_train_01.csv',\n",
       " 'datasets\\\\housing\\\\my_train_02.csv',\n",
       " 'datasets\\\\housing\\\\my_train_03.csv',\n",
       " 'datasets\\\\housing\\\\my_train_04.csv',\n",
       " 'datasets\\\\housing\\\\my_train_05.csv',\n",
       " 'datasets\\\\housing\\\\my_train_06.csv',\n",
       " 'datasets\\\\housing\\\\my_train_07.csv',\n",
       " 'datasets\\\\housing\\\\my_train_08.csv',\n",
       " 'datasets\\\\housing\\\\my_train_09.csv',\n",
       " 'datasets\\\\housing\\\\my_train_10.csv',\n",
       " 'datasets\\\\housing\\\\my_train_11.csv',\n",
       " 'datasets\\\\housing\\\\my_train_12.csv',\n",
       " 'datasets\\\\housing\\\\my_train_13.csv',\n",
       " 'datasets\\\\housing\\\\my_train_14.csv',\n",
       " 'datasets\\\\housing\\\\my_train_15.csv',\n",
       " 'datasets\\\\housing\\\\my_train_16.csv',\n",
       " 'datasets\\\\housing\\\\my_train_17.csv',\n",
       " 'datasets\\\\housing\\\\my_train_18.csv',\n",
       " 'datasets\\\\housing\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths  # train_filepaths = \"datasets/housing/my_train_*.csv\"로 쓸 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_files() : 파일 경로를 섞은 데이터 셋 반환\n",
    "# 섞지 않으려면 shuffle=False 옵션 사용\n",
    "\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in filepath_dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interleave() : 한 번에 다섯 개의 파일 한 줄씩 번갈아 읽기\n",
    "# 병렬화를 사용하진 않지만, num_parallel_calls로 병렬화 가능\n",
    "\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)   # 각 파일의 첫 줄은 열이름이므로 skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205'\n",
      "b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67'\n",
      "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
      "b'3.0217,22.0,4.983870967741935,1.1008064516129032,615.0,2.4798387096774195,38.76,-120.6,1.069'\n",
      "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n"
     ]
    }
   ],
   "source": [
    "# 데이터 살펴보기 \n",
    "# 순서는 랜덤이며, 바이트 스트링으로 되어 있음 (파싱하고 스케일 조정해야)\n",
    "\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.89175860e+00,  2.86245478e+01,  5.45593655e+00,  1.09963474e+00,\n",
       "         1.42428122e+03,  2.95886657e+00,  3.56464315e+01, -1.19584363e+02]),\n",
       " array([1.90927329e+00, 1.26409177e+01, 2.55038070e+00, 4.65460128e-01,\n",
       "        1.09576000e+03, 2.36138048e+00, 2.13456672e+00, 2.00093304e+00]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mean, X_std   # 훈련셋의 각 특성의 평균과 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "       # defs : [0, 0, 0, ..., 0, tf.Tensor]  -- X부분은 0으로 y는 텐서로\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "       # record_defaults는 기본값 배열(누락값의 기본값을 0으로, y는 기본값 X)\n",
    "    \n",
    "    x = tf.stack(fields[:-1])  # stack은 1D 텐서 배열을 반환\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([-0.8936168 ,  0.9789995 , -0.6810549 , -0.07264194, -0.5669866 ,\n",
       "        -0.39212456, -1.3522334 ,  1.2316071 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.205], dtype=float32)>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 데이터 적재와 전처리 합치기\n",
    "<img src='img/13_1.png' width='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재사용가능한코드를 만들기 위해 지금까지의 코드를 하나의 헬퍼 함수로 만들기\n",
    "# 적재, 전처리, 셔플링, 반복, 배치를 적용한 데이터 반환\n",
    "\n",
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                      n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                      n_parse_threads=5, batch_size=32):\n",
    "    \n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)  # 아래에서 살펴봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 프리패치\n",
    "- 마지막에 prefetch(1)를 호출하면 데이터셋은 항상 한 배치가 미리 준비되도록!  \n",
    "즉, 한 배치를 훈련하는 동안 다음 배치를 준비함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) tf.keras와 데이터셋 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바로 사용가능 \n",
    "\n",
    "# model.fit(train_set, epochs=10, validation_data=valid_set)\n",
    "# model . evaluate (test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 훈련 반복 수행하는 텐서플로 함수 직접 만들수도 있음\n",
    "\n",
    "# @tf.function\n",
    "# def train(model, optimizer, loss_fn, n_epochs, [...]):\n",
    "#     train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\n",
    "#     for X_batch, y_batch in train_set:\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = model(X_batch)\n",
    "#             main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "#             loss = tf.add_n([main_loss] + model.losses)\n",
    "#         grads = tape.gradient(loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradient(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TFRecord 포멧\n",
    "- csv 파일이 간단하고 편리해서 널리 통용되지만 대규모의 복잡한 (이미지나 오디오 등) 데이터는 지원 X => TFRecord 포맷\n",
    "- TFRecord : 크기가 다른 연속된 이진 레코드를 저장하는 단순한 이진 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter('datasets/my_data.tfrecord') as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = ['datasets/my_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (이하 생략) TFRecord 포맷이 필요할 때, 다시 공부해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 입력 특성 전처리\n",
    "- 모든 특성을 수치 특성으로 변환하고 정규화해야!\n",
    "- 도구를 이용해서 하거나, map 메서드를 이용하여 동적으로 처리하거나 전처리 층을 모델시킬 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lambda 층을 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(X_train, axis=0, keepdims=True)\n",
    "stds = np.std(X_train, axis=0, keepdims=True)\n",
    "eps = keras.backend.epsilon()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (means와 같은 전역 변수를 다루기보다) 사용자 정의 층 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer = Standardization()\n",
    "std_layer.adapt(data_sample)  # 이때, data_sample이 전체일 필요 X \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras.layers.Normalization 층 사용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 원-핫 벡터 사용하여 범주형 특성 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [ \"<1H OCEAN\" , \"INLAND\" , \"NEAR OCEAN\" , \"NEAR BAY\", \"ISLAND\" ]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "\n",
    "# tf.lookup.KeyValueTensorInitializer : 범주 리스트와 해당 인덱스를를 전달하여\n",
    "                                    # 룩업 테이블을 위해 초기화 객체를 만듦\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "\n",
    "\n",
    "# 어휘 사전에 없는 번주를 찾으면 룩업 테이블이 계산한 이 범주의 해시값을 이용해\n",
    "# oov(out-of-vocabulary) 버킷 중 하나에 할당 (있는 범주 다음부터! 여기서는 5, 6)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- oov 버킷을 사용하는 이유  \n",
    ": 범주의 개수가 많은 경우, 전체 범주 리스트를 구하는 것이 어려움  \n",
    "=> 샘플 데이터를 기반으로 어휘사전을 정의하고 데이터에 없는 범주를 oov 버킷에 추가 (oov 버킷이 충분해야!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 룩업 테이블 사용해 원-핫 벡터로 인코딩해보기\n",
    "\n",
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND'])\n",
    "cat_indices =  table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras.layers.TextVectorization 층 사용가능\n",
    "- 어휘 사전이 크면 __임베딩__을 사용하여 인코딩하는 것이 훨씬 효율적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 임베딩을 사용해 범주형 특성 인코딩하기\n",
    "- __임베딩__ : 범주를 표현하는 훈련 가능한 밀집 벡터 \n",
    "- 임베딩을 훈련할 수 있음 -> 비슷한 범주들 가깝게 만듦\n",
    "- 표현이 좋을수록 신경망이 정확한 예측을 만듦 (표현학습(17장 참고))\n",
    "\n",
    "   cf) 단어임베딩\n",
    "- 일반적으로 직접 단어임베딩을 훈련하는 것보다 사전훈련된 임베딩을 재사용하는 게 나음\n",
    "- King-Man, Queen-Woman과 같은 의미를 조직할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 임베딩 직접 구현해보기 (동작원리를 이용하기 위해) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 2), dtype=float32, numpy=\n",
       "array([[0.09632993, 0.9395069 ],\n",
       "       [0.09627533, 0.7377459 ],\n",
       "       [0.5443243 , 0.4047718 ],\n",
       "       [0.7686013 , 0.8976238 ],\n",
       "       [0.18245184, 0.8822814 ],\n",
       "       [0.50123084, 0.02763808],\n",
       "       [0.65395284, 0.3409189 ]], dtype=float32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embed_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.09632993, 0.9395069 ],\n",
       "       [0.09627533, 0.7377459 ],\n",
       "       [0.5443243 , 0.4047718 ],\n",
       "       [0.7686013 , 0.8976238 ],\n",
       "       [0.18245184, 0.8822814 ],\n",
       "       [0.50123084, 0.02763808],\n",
       "       [0.65395284, 0.3409189 ]], dtype=float32)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = tf.Variable(embed_init)\n",
    "embedding_matrix\n",
    "\n",
    "# 변수에 저장되며, 훈련과정에서 경사하강법으로 학습가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 예제 인코딩해보기\n",
    "\n",
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND'])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.7686013 , 0.8976238 ],\n",
       "       [0.50123084, 0.02763808],\n",
       "       [0.09627533, 0.7377459 ],\n",
       "       [0.09627533, 0.7377459 ]], dtype=float32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)  # [3, 5, 1, 1]해당 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 케라스에서 임베딩 행렬을 처리해주는 keras.layers.Embedding 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets,\n",
    "                                  output_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_2/Identity:0' shape=(None, 2) dtype=float32>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모두를 연결해보기 -> 임베딩을 학습하는 케라스 모델 만들 수 있음\n",
    "\n",
    "regular_inputs = keras.layers.Input(shape=[8])\n",
    "categories = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)  # Embedding 층과 동일한 역할\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories],\n",
    "                          outputs=[outputs])\n",
    "\n",
    "# 이 모델은 8개의 특성을 담은 입력과 하나의 범주형 입력을 가짐\n",
    "# Lambda 층을 사용해 범주의 인덱스를 찾은 다음 임베딩에서 이 인덱스를 찾음\n",
    "# 그다음 이 임베딩과 입반 입력을 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 케라스 전처리 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = keras.layers.Normalization()\n",
    "discretization = keras.layers.Discretization([...]) \n",
    "     # 연속적인 데이터를 구간으로 나누고 원-핫 벡터로 인코딩\n",
    "pipeline = keras.layers.PreprocessingStage([normalization, discretization])\n",
    "     # 여러개의 전처리층 합치기 (정규화하고 이산화하기)\n",
    "pipeline.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TextVectorization 층 : 단어 카운트 벡터를 출력  \n",
    " ex) 어휘사전이 ['and', 'basketball', 'more']이라면 'more and more' 텍스트는 벡터 [1, 0, 2]로 매핑됨 (단어의 개수)  \n",
    " => 이런 텍스트 표현은 순서를 완전히 무시하기 때문에 __BOW__라 부름\n",
    " \n",
    "    cf) TF-IDF   \n",
    "    = tf * log(1 + ${전체 샘플 수} \\over {1+단어가 등장하는 샘플 수}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TF 변환\n",
    "- 전처리 연산을 한 번만 정의하도록 하는 TF 변환\n",
    "- 텐서플로 모델 상품화의 엔드-투-엔드(end-to-end) 플랫폼 TFX의 일부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 텐서플로 데이터셋(TFDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(name='mnist')\n",
    "mnist_train, mnist_test = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items['image'], items['label']))\n",
    "                                           # 튜플로 만들어주기\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 간단히 (as_supervies=True 옵션 사용)\n",
    "\n",
    "dataset = tfds.load(name='mnist', batch_size=32, as_supervised=True)\n",
    "mnist_train = dataset['train'].prefetch(1)\n",
    "\n",
    "[...]\n",
    "model.fit(mnist_train, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
